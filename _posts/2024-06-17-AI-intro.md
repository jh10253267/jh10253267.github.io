---
layout: post
title:  "AI intro"
author: 악어새62
categories: [ TIL, AI ]
image: assets/images/5.jpg
tags: [AI, 경사하강법]
use_math: true
---
## 개요

처음에는 AI분야를 공부해보고 싶었다.  
컴퓨터는 마법이 아니다. 코딩 공부를 하며 컴퓨터에 대해 이해하면서 결국 컴퓨터도 최소 단위로 작동하고 이 동작들이 모여서 우리가 생각하는 컴퓨터로써 기능을 하는 거구나 하는 생각이 들었다.

AI에 관심이 가게 된 계기도 비슷하다. 영화나 매체에서 나오는 AI는 마법같았다. gpt는 물어보면 뭐든지 알려준다. 마법같지만 아니다. 도저히 어떻게 하는건지 감도 안온다. 이럴 때 흥미가 생기곤한다.

## 머신러닝, AI, 딥러닝

> "지적인 기계, 특히 지적인 컴퓨터 프로그램을 작성하는 과학과 기술"

인공지능에 대한 연구는 1950년대부터 시작되었다고 한다. 다른 분야도 그렇듯이 AI도 연관된 다른 분야의 발전과 함께 발전했다.

* 1세대 연구는 사람이 하고 싶은 것을 프로그래밍으로 구현하는 것 처럼 프로그램으로 지능을 구현할 수 있다고 생각한 시기였다.  
* 2세대 연구는 하드웨어의 비약적인 발전 시기와 겹친다. 여러 분야의 전분가들의 지식을 가르쳐 넣는 AI로 강화학습 개발 기법이 연구되었다.  
* 3세대 연구는 딥러닝이 주가 된다. 대량의 데이터로부터 스스로 학습한다. 인터넷의 발전으로 대량의 데이터가 생기고 공유되었기 때문이다.

시대가 지남에 따라 사람이 기계에게 가르쳐준다에서 기계가 스스로 배운다는 컨셉으로 바뀌어왔다. 이를 머신러닝이라고 부른다.

예를 들어 강아지와 고양이 사진을 보고 판별하는 프로그램의 경우 1~2세대의 AI는 사진을 보여주고 "고양이"라는 답을 알려주었다.  
2~3세대의 AI는 컴퓨터가 해석하고 답을 낸다.

머신러닝은 여러 곳에서 쓰이지만 주로 예측, 식별, 분류에 쓰인다.  
예를 들어 컨베이어벨트에 놓인 AI는 물품을 식별하고 적절히 분류한다.

## 지도학습, 비지도학습

지도학습은 정답이 부여된 훈련 데이터를 분석하는 것으로 모델을 확정하고 이를 이용하여 새로운 데이터를 예측, 식별, 분류한다.  
이 때 훈련 데이터중 정답 부분을 정답 레이블이라고 한다.

비지도학습은 정답 레이블에 해당하는 부분이 없고 주어진 데이터의 성질에 기초해 예측, 식별, 분류한다.  
강화학습은 시행착오를 통해 가치를 최대화하는 행동을 학습한다.

## 기본적인 알고리즘

### 모델의 최적화와 최소제곱법

데이터를 분석하기 위한 모델은 주어진 데이터를 잘 설명할 수 있도록 만들어진다. 그러나 현실에서는 모델이 데이터를 100%설명하는 것은 어렵다.  
따라서 모델이 설명할 수 없는 부분을 최소화하는 모델의 파라미터를 결정한다. 이를 최적화라고 한다.

대표적인 방법이 바로 최소제곱법이다.

예를 들어 n개의 데이터 중 k번째 요소의 실제값에 대해 모델로부터 산출되는 예측값이 있을 것이다. 이 때 실제 값과 예측값의 차이를 오차라고 하고 이를 제곱한 것을 제곱오차라고 한다.

이렇게 제곱 오차를 구한 뒤 모두 더한다. 이 때 오차의 총 합을 최소화하기위해 모델의 파라미터를 결정하는 방법을 바로 최소제곱법이라고 한다.  
그리고 이 총합을 최소화하는 파라미터를 가진 모델이 최적이라는 논리다.

오차를 그냥 더하지 않고 제곱하여 더하는 이유는 크게 세 가지로 설명할 수 있다.
1. 오차의 부호 상쇄: 예를 들어 데이터 A의 오차는 +3이고 B는 -3이라고 했을 때 이를 단순히 더하면 상쇄되어 오차의 합은 0이 된다. 분명히 오차가 있는데 말이다.
2. 패널티: 위에서 살펴본 대로 모델이 설명할 수 없는 부분을 최소화하기 위해 최적화를 수행하는데 큰 오차가 발생한 지점은 모델이 잘 설명하지 못하는 영역으로 이 영역을 무시하면 특정 부분만 잘 맞추고 다른 부분에선 오류를 범하게 되기 때문이다. 제곱을 하면 큰 값은 작은 값에 비해 더 크게 증가하게 된다.
3. 수학적 편리성: 제곱을 사용하면 미분이 가능해지고 수학적으로 최소값을 찾기 더 쉬워지는 장점이 있다.

간단한 예로 회귀분석이 있다. 수학에서의 외삽개념이다.  
실제 관측된 데이터가 있고 그 바깥까지 그래프를 이어서 예측하는 방법이다. 

예를 들어 어느 나라의 경제성장률이 있고 이를 통해 다음 년도의 경제성장률을 예측하는 식이다.

| x | y |
| -- | -- |
| 1 | 13.3 |
| 2 | 15.8 |
| 3 | 19.4 |
| 4 | 22.3 |

여기서 성장률을 x와 y의 1차식인 y = ax + b로 나타낼 수 있다고 가정한다.

k년도의 경제 성장률은 y = ak + b로 나타낼 수 있다.  
이제 최소제곱법을 이용하여 파라미터를 결정한다.


$$e_{k} = (y_{k} - \hat{y}_{k})^2 = (y_{k} - (ax_{k} + b))^2$$

이 식에 대한 해가 최소화될 때 다음의 관계가 성립한다.  

$$\frac{\delta E}{\delta a} = 0, \frac{\delta E}{\delta b} = 0$$


이를 계산해보면 다음과 같다.

$$ a = 3.06, b = 10.05 $$

$$ y = 3.06x + 10.5 $$

만약 5차 년도의 경제성장률을 구한다면 약 25%가 나온다.

## 경사하강법

$$z = f(x,y)$$

오차를 최소로 한다 -> 그래프의 최솟점에서의 파라미터를 결정한다.  
최솟점을 찾는 방법은 함수를 미분하여 기울기가 0이 되는 지점을 찾는 것이다.  
기울기가 0이라는 것은 기울기가 +에서 -로 바뀌는 지점, -에서 +로 바뀌는 지점을 의미하고 이는 극점을 의미한다.  
따라서 미분을 하여 최소점을 찾는다면 이 때의 파라미터가 오차를 최소한으로 하는 학습 펙터인 것이다. 그러나 실제 목적 함수에서는 파라미터가 굉장히 많아지고 단번에 기울기가 0이 되는 파라미터를 찾기 어려워진다. 따라서 단번에 찾는 것이 아닌 단계적으로 찾는 방법을 생각해내게 되는데 그게 바로 경사 하강법이다.

함수의 그래프를 경사면이라고 가정한다. 어느 점에 탁구공을 놓고 손을 떼어본다. 공은 경사가 가장 급한 곳을 따라서 구른다.

조금 전진한 후 탁구공을 잡고 다시 놓고 굴린다. 이렇게 하면 탁구공은 가장 경사가 급한 루트를 따라 그래프의 바닥에 닿을 것이다.

경사하강법은 탁구공의 움직임을 흉내낸 것이다.  
이를 수식으로 살펴보자면 다음과 같다.

$$z = f(x,y)$$

함수에서 x를 델타 x만큼 변화시킬 때의 함수 값의 변화를 델타 z라고 하면 이렇게 표현할 수 있다.

$$\Delta z = f(x + \Delta x, y + \Delta y) - f(x, y)$$

이는 미분 근사공식에 따라 다음과 같이 변형된다.

$$\Delta z = \frac{\delta f(x, y)}{\delta x} \Delta x + \frac{\delta f(x, y)}{\delta y} \Delta y$$

우변을 보면 벡터의 내적 형태를 하고있다.

따라서 위의 식은 다음과 같다.

$$\left(\frac{\delta f(x, y)}{\delta x} , \frac{\delta f(x, y)}{\delta y}\right), (\Delta x, \Delta y)$$

왼쪽의 벡터는 함수 f(x, y)의 점 x, y에서의 기울기다.

$$\left(\frac{\delta f(x, y)}{\delta x} , \frac{\delta f(x, y)}{\delta y}\right)$$

즉 x와 y를 델타 x, y만큼 변화시킬 때 함수의 변화인 델타 z는 두 벡터의 내적으로 표현된다.  
여기서 델타 Z가 0이라는 것은 즉 낙차가 없다는 의미이며 이는 최소점에 도달했다는 의미이다. 따라서 델타 Z가 최소가 되도록 일종의 압력을 가하는 것이다.  
내적이 최소가 되는 조건은 두 벡터가 180도 관계에 있을 경우로 식으로 표현하자면 다음과 같다.

$$(\Delta x, \Delta y) = -\eta\left(\frac{\delta f(x, y)}{\delta x} , \frac{\delta f(x, y)}{\delta y}\right)$$

그레디언트 벡터와 반대 방향으로 조금씩 진행하다보면 최소값을 찾을 수 있는 것이다.

예제 1
$$ y = x^2 $$

초기 값을 5로 시작해보자. 학습율인 에타는 0.1로 정했다.  
그레디언트는 2x가 되고 5를 대입해보면 10*0.1로 1이 된다. 따라서 x = -1만큼 공을 이동시킨다.  
이 과정을 반복하다보면 점진적으로 x가 0으로 수렴하며 함수 값도 최소화된다.
![image](https://github.com/user-attachments/assets/462845aa-9427-4939-9539-a2ca7ab04932)

예제 2

$$ z = x^2 + y^2 $$

x, y가 1부터 1+ 델타 x, 2부터 델타 2+y로 변할 때 이 함수가 가장 크게 감소할 때 벡터 (델타 x, 델타 y)를 구하시오.

$$(\Delta x, \Delta y) = -\eta\left(\frac{\delta f(x, y)}{\delta x} , \frac{\delta f(x, y)}{\delta y}\right)$$

$$ (\Delta x, \Delta y) = -\eta(2, 4)$$

경사하강법을 사용할 때 주의해야할 점은 스탭사이즈를 적절하게 정하는 것이다. 예를 들어 사람이 이동할 때 보폭이 크다면 웅덩이를 밟지 않고 건너 뛰게 될 것이고 너무 작다면 웅덩이에서 탈출하지 못할 수 있다.